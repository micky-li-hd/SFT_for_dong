{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import copy\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import logging\n",
    "import pathlib\n",
    "from typing import Dict, Optional, Sequence, List\n",
    "import time\n",
    "import torch, gc\n",
    "import glob\n",
    "import transformers\n",
    "import tokenizers\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image, ImageFile\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from pathlib import Path\n",
    "from datasets.utils.logging import set_verbosity_info\n",
    "from transformers import logging as tf_logging\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoProcessor\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sample(sample):\n",
    "    try:\n",
    "        metadata = sample[\"json\"]\n",
    "        return {\n",
    "            \"caption\": metadata.get(\"caption\"),\n",
    "            \"cot\": metadata.get(\"cot\"),\n",
    "            \"aspect_ratio\": metadata.get(\"aspect_ratio\"),\n",
    "            \"img_index\": metadata.get(\"img_index\")\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing sample: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from datasets import concatenate_datasets\n",
    "import glob\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "class LazySupervisedMixDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: str,\n",
    "        processor: AutoProcessor,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_files = glob.glob(os.path.join(data_path, \"*.tar\"))  # 保存 tar 文件列表\n",
    "        train_datasets = []\n",
    "        self.offsets = [0]  # 记录每个 tar 文件的起始索引\n",
    "\n",
    "        # 逐个加载并处理 tar 文件\n",
    "        for data_file in self.data_files:\n",
    "            raw_dataset = load_dataset(\"webdataset\", data_files=[data_file], split=\"train\", num_proc=128)\n",
    "            train_dataset = raw_dataset.map(process_sample).filter(lambda x: x is not None)\n",
    "            train_datasets.append(train_dataset)\n",
    "            self.offsets.append(self.offsets[-1] + len(train_dataset))  # 累积样本数\n",
    "\n",
    "        # 合并数据集\n",
    "        if len(train_datasets) > 1:\n",
    "            self.list_data_dict = concatenate_datasets(train_datasets)\n",
    "        else:\n",
    "            self.list_data_dict = train_datasets[0]\n",
    "\n",
    "        self.processor = processor\n",
    "\n",
    "    # def process_sample(self, sample):\n",
    "    #     # 示例处理逻辑，确保返回有效样本\n",
    "    #     if 'caption' not in sample or 'cot' not in sample:\n",
    "    #         return None\n",
    "    #     return sample\n",
    "\n",
    "    def get_tar_info(self, index: int):\n",
    "        \"\"\"根据全局索引定位到 tar 文件和文件内偏移量\"\"\"\n",
    "        for i in range(len(self.offsets) - 1):\n",
    "            if self.offsets[i] <= index < self.offsets[i + 1]:\n",
    "                tar_index = i  # tar 文件索引\n",
    "                file_index = index - self.offsets[i]  # 文件内偏移量\n",
    "                return self.data_files[tar_index], file_index\n",
    "        raise IndexError(f\"Index {index} out of range\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_data_dict)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        sources = self.list_data_dict[i]\n",
    "\n",
    "        conversation = [\n",
    "            {\"role\": \"<|User|>\", \"content\": sources['caption']},\n",
    "            {\"role\": \"<|Assistant|>\", \"content\": f\"{sources['cot']}<begin_of_image><end_of_image>\"},\n",
    "        ]\n",
    "        system_prompt = \"You are an assistant that creates images from descriptions. First, describe the image in detail, then generate it.\"\n",
    "        prompt = self.processor.apply_sft_template_for_multi_turn_prompts(\n",
    "            conversations=conversation,\n",
    "            sft_format=self.processor.sft_format,\n",
    "            system_prompt=system_prompt,\n",
    "        )\n",
    "\n",
    "        # Tokenize prompt\n",
    "        text_ids = self.processor.tokenizer.encode(prompt)\n",
    "        all_ids = text_ids[:-2] + sources['img_index'] + text_ids[-2:]\n",
    "        all_ids = torch.LongTensor(all_ids)\n",
    "\n",
    "        # 构建图像 token 的 mask\n",
    "        all_image_ids_mask = torch.zeros(all_ids.shape, dtype=torch.bool)\n",
    "        all_image_ids_mask[-len(sources['img_index'])-2:-2] = True\n",
    "\n",
    "        # 找到 Assistant 回答开始的位置\n",
    "        try:\n",
    "            assistant_start_token_id = self.processor.tokenizer.encode(\"<|Assistant|>\")[0]\n",
    "            assistant_start_index = text_ids.index(assistant_start_token_id)\n",
    "        except (ValueError, IndexError):\n",
    "            assistant_start_index = 0\n",
    "\n",
    "        assistant_ids_mask = torch.zeros(all_ids.shape, dtype=torch.bool)\n",
    "        assistant_ids_mask[assistant_start_index:] = True\n",
    "\n",
    "        # 构造输入和标签\n",
    "        input_ids = all_ids[:-1]\n",
    "        text_ids_mask = (all_image_ids_mask[:-1] == False)\n",
    "        image_ids_mask = all_image_ids_mask[:-1]\n",
    "        label_ids = all_ids[1:]\n",
    "        label_text_ids_mask = assistant_ids_mask[1:] & (all_image_ids_mask[1:] == False)\n",
    "        label_image_ids_mask = assistant_ids_mask[1:] & all_image_ids_mask[1:]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"label_ids\": label_ids,\n",
    "            \"text_ids_mask\": text_ids_mask,\n",
    "            \"image_ids_mask\": image_ids_mask,\n",
    "            \"label_text_ids_mask\": label_text_ids_mask,\n",
    "            \"label_image_ids_mask\": label_image_ids_mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16384\n",
      "Padding ID: 100015\n"
     ]
    }
   ],
   "source": [
    "from janus.models.processing_vlm import VLChatProcessor\n",
    "processor: VLChatProcessor = VLChatProcessor.from_pretrained(\"deepseek-ai/Janus-Pro-7B\")\n",
    "tokenizer = processor.tokenizer\n",
    "print(tokenizer.model_max_length)\n",
    "padding_id = tokenizer.pad_token_id\n",
    "print(f\"Padding ID: {padding_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = glob.glob(os.path.join(\"/home/v-haodongli/mnt/v-haodongli-container/cot_output_test_train\", \"*.tar\"))\n",
    "# train_dataset = load_dataset(\"webdataset\", data_files=data_files, split=\"train\", streaming=True ,num_proc=8)\n",
    "train_dataset = load_dataset(\"webdataset\", data_files=data_files, split=\"train\", num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['json', '__key__', '__url__'],\n",
       "    num_rows: 2819186\n",
       "})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'json': {'aspect_ratio': '3:2',\n",
       "  'caption': 'Hoodoos Rule by Charlene Reinauer',\n",
       "  'cot': 'The image depicts a landscape dominated by hoodoos, which are tall, thin rock formations with a conical or columnar shape. The hoodoos are primarily orange and brown in color, with some darker patches indicating variations in rock composition or weathering. The formations are closely packed together, creating a dense cluster. The background features a forested area with green trees, providing a contrast to the reddish hues of the hoodoos. The sky is not visible, focusing the viewer\\'s attention on the rock formations and the forest. The style of the image is a natural, outdoor scene, likely taken during the day given the lighting and shadows. The image is titled \"Hoodoos Rule\" by Charlene Reinauer, suggesting a focus on the unique geological features and the artist\\'s perspective.',\n",
       "  'img_index': [4428,\n",
       "   14063,\n",
       "   13880,\n",
       "   14031,\n",
       "   10621,\n",
       "   5473,\n",
       "   6442,\n",
       "   10630,\n",
       "   6781,\n",
       "   2956,\n",
       "   15081,\n",
       "   12544,\n",
       "   1378,\n",
       "   4294,\n",
       "   4622,\n",
       "   9347,\n",
       "   10468,\n",
       "   14737,\n",
       "   12574,\n",
       "   15587,\n",
       "   2994,\n",
       "   2715,\n",
       "   6222,\n",
       "   2521,\n",
       "   10117,\n",
       "   882,\n",
       "   348,\n",
       "   5824,\n",
       "   9176,\n",
       "   2126,\n",
       "   4168,\n",
       "   7242,\n",
       "   7365,\n",
       "   9243,\n",
       "   15691,\n",
       "   15562,\n",
       "   5713,\n",
       "   12052,\n",
       "   6597,\n",
       "   14306,\n",
       "   938,\n",
       "   2957,\n",
       "   4556,\n",
       "   10039,\n",
       "   9430,\n",
       "   13348,\n",
       "   1464,\n",
       "   762,\n",
       "   9446,\n",
       "   4534,\n",
       "   3695,\n",
       "   967,\n",
       "   11279,\n",
       "   4534,\n",
       "   273,\n",
       "   2371,\n",
       "   10393,\n",
       "   8389,\n",
       "   3000,\n",
       "   14797,\n",
       "   8137,\n",
       "   7720,\n",
       "   813,\n",
       "   6826,\n",
       "   15311,\n",
       "   167,\n",
       "   8022,\n",
       "   11639,\n",
       "   7297,\n",
       "   8710,\n",
       "   3339,\n",
       "   8375,\n",
       "   135,\n",
       "   3482,\n",
       "   405,\n",
       "   10486,\n",
       "   15221,\n",
       "   12295,\n",
       "   12342,\n",
       "   3428,\n",
       "   2975,\n",
       "   15691,\n",
       "   7805,\n",
       "   1768,\n",
       "   1749,\n",
       "   42,\n",
       "   2696,\n",
       "   9343,\n",
       "   11353,\n",
       "   6684,\n",
       "   12198,\n",
       "   8155,\n",
       "   9725,\n",
       "   4825,\n",
       "   8817,\n",
       "   13715,\n",
       "   3964,\n",
       "   16193,\n",
       "   12124,\n",
       "   9819,\n",
       "   5875,\n",
       "   14323,\n",
       "   597,\n",
       "   604,\n",
       "   1553,\n",
       "   11313,\n",
       "   12925,\n",
       "   5671,\n",
       "   15206,\n",
       "   4323,\n",
       "   14421,\n",
       "   710,\n",
       "   9541,\n",
       "   10274,\n",
       "   846,\n",
       "   9373,\n",
       "   10069,\n",
       "   767,\n",
       "   11470,\n",
       "   6821,\n",
       "   10107,\n",
       "   7956,\n",
       "   5654,\n",
       "   2781,\n",
       "   9265,\n",
       "   10138,\n",
       "   14666,\n",
       "   13694,\n",
       "   14912,\n",
       "   14681,\n",
       "   4230,\n",
       "   7393,\n",
       "   13656,\n",
       "   5742,\n",
       "   15464,\n",
       "   11380,\n",
       "   1251,\n",
       "   13174,\n",
       "   4610,\n",
       "   11476,\n",
       "   7458,\n",
       "   7055,\n",
       "   8508,\n",
       "   11476,\n",
       "   7875,\n",
       "   12791,\n",
       "   5570,\n",
       "   7924,\n",
       "   11379,\n",
       "   6999,\n",
       "   1179,\n",
       "   14814,\n",
       "   15347,\n",
       "   7033,\n",
       "   14580,\n",
       "   10231,\n",
       "   15187,\n",
       "   9623,\n",
       "   15976,\n",
       "   2775,\n",
       "   8050,\n",
       "   7502,\n",
       "   15883,\n",
       "   5606,\n",
       "   12049,\n",
       "   10447,\n",
       "   8883,\n",
       "   15408,\n",
       "   640,\n",
       "   15204,\n",
       "   12416,\n",
       "   14993,\n",
       "   4291,\n",
       "   1628,\n",
       "   13647,\n",
       "   9325,\n",
       "   869,\n",
       "   5231,\n",
       "   16265,\n",
       "   10014,\n",
       "   10033,\n",
       "   12776,\n",
       "   1514,\n",
       "   14662,\n",
       "   7585,\n",
       "   16127,\n",
       "   7692,\n",
       "   917,\n",
       "   2012,\n",
       "   6831,\n",
       "   5780,\n",
       "   1405,\n",
       "   12826,\n",
       "   8804,\n",
       "   7643,\n",
       "   5702,\n",
       "   9517,\n",
       "   228,\n",
       "   13736,\n",
       "   3394,\n",
       "   9621,\n",
       "   2793,\n",
       "   9798,\n",
       "   924,\n",
       "   12749,\n",
       "   15579,\n",
       "   2665,\n",
       "   4961,\n",
       "   7404,\n",
       "   14444,\n",
       "   15991,\n",
       "   14789,\n",
       "   2084,\n",
       "   9581,\n",
       "   2773,\n",
       "   5439,\n",
       "   1022,\n",
       "   2825,\n",
       "   8844,\n",
       "   15836,\n",
       "   3393,\n",
       "   6299,\n",
       "   5962,\n",
       "   1916,\n",
       "   10893,\n",
       "   13756,\n",
       "   11563,\n",
       "   12227,\n",
       "   14483,\n",
       "   11006,\n",
       "   10167,\n",
       "   15343,\n",
       "   1289,\n",
       "   1687,\n",
       "   14761,\n",
       "   10292,\n",
       "   7282,\n",
       "   5413,\n",
       "   2242,\n",
       "   8106,\n",
       "   7935,\n",
       "   8108,\n",
       "   6182,\n",
       "   3208,\n",
       "   9752,\n",
       "   12117,\n",
       "   3196,\n",
       "   7482,\n",
       "   8027,\n",
       "   921,\n",
       "   11878,\n",
       "   2785,\n",
       "   8726,\n",
       "   4526,\n",
       "   1569,\n",
       "   7208,\n",
       "   1623,\n",
       "   7165,\n",
       "   12159,\n",
       "   7937,\n",
       "   7992,\n",
       "   14692,\n",
       "   14951,\n",
       "   10894,\n",
       "   13527,\n",
       "   5062,\n",
       "   11568,\n",
       "   2330,\n",
       "   6119,\n",
       "   10546,\n",
       "   9313,\n",
       "   308,\n",
       "   15628,\n",
       "   3163,\n",
       "   13698,\n",
       "   4673,\n",
       "   5257,\n",
       "   7725,\n",
       "   6461,\n",
       "   2007,\n",
       "   296,\n",
       "   3766,\n",
       "   14741,\n",
       "   7353,\n",
       "   3892,\n",
       "   1619,\n",
       "   3404,\n",
       "   13759,\n",
       "   14507,\n",
       "   14132,\n",
       "   16174,\n",
       "   14852,\n",
       "   13854,\n",
       "   1175,\n",
       "   10251,\n",
       "   10954,\n",
       "   9287,\n",
       "   9330,\n",
       "   13669,\n",
       "   13105,\n",
       "   6310,\n",
       "   1249,\n",
       "   14952,\n",
       "   5099,\n",
       "   15388,\n",
       "   13157,\n",
       "   9731,\n",
       "   4891,\n",
       "   1985,\n",
       "   12701,\n",
       "   11537,\n",
       "   5244,\n",
       "   12616,\n",
       "   3407,\n",
       "   7327,\n",
       "   3998,\n",
       "   9001,\n",
       "   14814,\n",
       "   15312,\n",
       "   1340,\n",
       "   3947,\n",
       "   11395,\n",
       "   7547,\n",
       "   16211,\n",
       "   576,\n",
       "   16191,\n",
       "   2812,\n",
       "   3617,\n",
       "   9312,\n",
       "   8128,\n",
       "   12731,\n",
       "   5254,\n",
       "   11768,\n",
       "   14073,\n",
       "   10978,\n",
       "   16209,\n",
       "   5231,\n",
       "   3752,\n",
       "   5559,\n",
       "   8317,\n",
       "   13596,\n",
       "   7838,\n",
       "   13995,\n",
       "   6825,\n",
       "   4277,\n",
       "   14588,\n",
       "   14609,\n",
       "   7144,\n",
       "   3698,\n",
       "   8008,\n",
       "   15887,\n",
       "   7582,\n",
       "   3301,\n",
       "   13902,\n",
       "   5891,\n",
       "   14314,\n",
       "   6482,\n",
       "   9951,\n",
       "   1704,\n",
       "   7519,\n",
       "   14551,\n",
       "   6461,\n",
       "   8731,\n",
       "   12854,\n",
       "   8959,\n",
       "   5604,\n",
       "   11879,\n",
       "   5010,\n",
       "   7883,\n",
       "   15070,\n",
       "   1753,\n",
       "   10222,\n",
       "   11452,\n",
       "   8219,\n",
       "   4230,\n",
       "   6257,\n",
       "   14852,\n",
       "   10506,\n",
       "   5125,\n",
       "   12106,\n",
       "   1818,\n",
       "   12198,\n",
       "   2568,\n",
       "   15810,\n",
       "   13818,\n",
       "   13171,\n",
       "   9520,\n",
       "   13930,\n",
       "   892,\n",
       "   3037,\n",
       "   12365,\n",
       "   8902,\n",
       "   29,\n",
       "   6266,\n",
       "   4671,\n",
       "   5122,\n",
       "   15184,\n",
       "   13839,\n",
       "   16043,\n",
       "   9655,\n",
       "   5566,\n",
       "   12786,\n",
       "   8772,\n",
       "   15464,\n",
       "   305,\n",
       "   8862,\n",
       "   2330,\n",
       "   640,\n",
       "   9842,\n",
       "   13459,\n",
       "   14217,\n",
       "   4704,\n",
       "   10398,\n",
       "   9181,\n",
       "   3335,\n",
       "   1116,\n",
       "   125,\n",
       "   9055,\n",
       "   6942,\n",
       "   5799,\n",
       "   4354,\n",
       "   8083,\n",
       "   11191,\n",
       "   3194,\n",
       "   12213,\n",
       "   10270,\n",
       "   7136,\n",
       "   14733,\n",
       "   12050,\n",
       "   15105,\n",
       "   6251,\n",
       "   9019,\n",
       "   6189,\n",
       "   13145,\n",
       "   4573,\n",
       "   2249,\n",
       "   12154,\n",
       "   1128,\n",
       "   14852,\n",
       "   6421,\n",
       "   13536,\n",
       "   15132,\n",
       "   16063,\n",
       "   7341,\n",
       "   5676,\n",
       "   2898,\n",
       "   15409,\n",
       "   724,\n",
       "   8680,\n",
       "   1443,\n",
       "   6025,\n",
       "   2665,\n",
       "   762,\n",
       "   5673,\n",
       "   7559,\n",
       "   8064,\n",
       "   16222,\n",
       "   13105,\n",
       "   9893,\n",
       "   12403,\n",
       "   1418,\n",
       "   5412,\n",
       "   3867,\n",
       "   582,\n",
       "   31,\n",
       "   5445,\n",
       "   13090,\n",
       "   9004,\n",
       "   5792,\n",
       "   5072,\n",
       "   14309,\n",
       "   4578,\n",
       "   4090,\n",
       "   10223,\n",
       "   8349,\n",
       "   14610,\n",
       "   1606,\n",
       "   2010,\n",
       "   7445,\n",
       "   10190,\n",
       "   7300,\n",
       "   4556,\n",
       "   13572,\n",
       "   12887,\n",
       "   16205,\n",
       "   5666,\n",
       "   111,\n",
       "   16153,\n",
       "   7284,\n",
       "   5234,\n",
       "   13786,\n",
       "   10415,\n",
       "   4030,\n",
       "   1339,\n",
       "   6559,\n",
       "   2201,\n",
       "   5874,\n",
       "   10300,\n",
       "   11360,\n",
       "   9450,\n",
       "   4307,\n",
       "   1800,\n",
       "   12913,\n",
       "   8332,\n",
       "   7011,\n",
       "   15129,\n",
       "   2232,\n",
       "   9623,\n",
       "   2833,\n",
       "   9937,\n",
       "   9518,\n",
       "   1759,\n",
       "   9495,\n",
       "   5121,\n",
       "   10225,\n",
       "   4309,\n",
       "   9431,\n",
       "   5648,\n",
       "   16303,\n",
       "   3287,\n",
       "   11583,\n",
       "   7665,\n",
       "   3127,\n",
       "   13371,\n",
       "   9531,\n",
       "   7162,\n",
       "   2949,\n",
       "   10812,\n",
       "   462,\n",
       "   1838,\n",
       "   11676,\n",
       "   5079,\n",
       "   2942,\n",
       "   7471,\n",
       "   14437,\n",
       "   364,\n",
       "   1318,\n",
       "   8731,\n",
       "   3535,\n",
       "   9181,\n",
       "   2390,\n",
       "   15771,\n",
       "   11994,\n",
       "   16085,\n",
       "   15055,\n",
       "   10501,\n",
       "   5987,\n",
       "   7363,\n",
       "   15269,\n",
       "   7407,\n",
       "   7582,\n",
       "   2752,\n",
       "   9969,\n",
       "   4017,\n",
       "   2514,\n",
       "   3520,\n",
       "   9926,\n",
       "   9710,\n",
       "   7581,\n",
       "   6003,\n",
       "   519,\n",
       "   6597,\n",
       "   14780,\n",
       "   4775,\n",
       "   13469,\n",
       "   13447,\n",
       "   11394,\n",
       "   8578,\n",
       "   278,\n",
       "   10290,\n",
       "   7782,\n",
       "   16191,\n",
       "   1818,\n",
       "   125,\n",
       "   10059,\n",
       "   12165]},\n",
       " '__key__': '00001_00001',\n",
       " '__url__': '/home/v-haodongli/mnt/v-haodongli-container/cot_output_test_train/00001.tar'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sample(sample):\n",
    "    try:\n",
    "        metadata = sample[\"json\"]\n",
    "        return {\n",
    "            \"caption\": metadata.get(\"caption\"),\n",
    "            \"cot\": metadata.get(\"cot\"),\n",
    "            \"aspect_ratio\": metadata.get(\"aspect_ratio\"),\n",
    "            \"img_index\": metadata.get(\"img_index\")\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing sample: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Sequence\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import AutoProcessor\n",
    "import glob\n",
    "import transformers\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "import glob\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Sequence, Dict, Any\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset:\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "    tokenizer: PreTrainedTokenizer\n",
    "    processor: Any  # 替换为你的具体 processor 类型（如 VLMProcessor）\n",
    "    max_length: int = 1024\n",
    "    IGNORE_INDEX: int = -100\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids_list = []\n",
    "        labels_list = []\n",
    "        text_ids_mask_list = []\n",
    "        image_ids_mask_list = []\n",
    "        label_text_ids_mask_list = []\n",
    "        label_image_ids_mask_list = []\n",
    "\n",
    "        for instance in instances:\n",
    "            # 提取 caption 和 img_index\n",
    "            try:\n",
    "                json_data = instance['json']\n",
    "                caption = json_data['caption']\n",
    "                cot = json_data['cot']  # 注意这里新增了 cot 字段\n",
    "                img_index = json_data['img_index']  # list of int 或者 tensor\n",
    "            except KeyError as e:\n",
    "                raise ValueError(f\"Missing key in instance: {e}\")\n",
    "\n",
    "            # 构造 conversation\n",
    "            conversation = [\n",
    "                {\"role\": \"<|User|>\", \"content\": caption},\n",
    "                {\"role\": \"<|Assistant|>\", \"content\": f\"{cot}<begin_of_image><end_of_image>\"},\n",
    "            ]\n",
    "            system_prompt = \"You are an assistant that creates images from descriptions. First, describe the image in detail, then generate it.\"\n",
    "\n",
    "            # 使用 self.processor 来生成 prompt\n",
    "            prompt = self.processor.apply_sft_template_for_multi_turn_prompts(\n",
    "                conversations=conversation,\n",
    "                sft_format=self.processor.sft_format,\n",
    "                system_prompt=system_prompt,\n",
    "            )\n",
    "\n",
    "            # Tokenize prompt\n",
    "            text_ids = self.tokenizer.encode(prompt)\n",
    "\n",
    "            # 插入图像 token ID\n",
    "            all_ids = text_ids[:-2] + img_index + text_ids[-2:]\n",
    "            all_ids = torch.LongTensor(all_ids)\n",
    "\n",
    "            # 构建图像 token 的 mask\n",
    "            all_image_ids_mask = torch.zeros(len(all_ids), dtype=torch.bool)\n",
    "            all_image_ids_mask[-len(img_index)-2:-2] = True\n",
    "\n",
    "            # 找到 Assistant 回答开始的位置\n",
    "            try:\n",
    "                assistant_start_token_id = self.tokenizer.encode(\"<|Assistant|>\")[0]\n",
    "                assistant_start_index = (all_ids == assistant_start_token_id).nonzero(as_tuple=True)[0][0].item()\n",
    "            except Exception:\n",
    "                assistant_start_index = 0\n",
    "\n",
    "            # 构造各类 mask\n",
    "            assistant_mask = torch.zeros(len(all_ids), dtype=torch.bool)\n",
    "            assistant_mask[assistant_start_index:] = True\n",
    "\n",
    "            # 构造 input 和 label\n",
    "            input_ids = all_ids[:-1]\n",
    "            label_ids = all_ids[1:]\n",
    "\n",
    "            text_mask = (all_image_ids_mask[:-1] == False)\n",
    "            image_mask = all_image_ids_mask[:-1]\n",
    "\n",
    "            label_text_mask = assistant_mask[1:] & (all_image_ids_mask[1:] == False)\n",
    "            label_image_mask = assistant_mask[1:] & all_image_ids_mask[1:]\n",
    "\n",
    "            # 只保留 label 中需要的部分，其他设为 IGNORE_INDEX\n",
    "            label_ids[~label_text_mask] = self.IGNORE_INDEX\n",
    "\n",
    "            # 添加进列表\n",
    "            input_ids_list.append(input_ids)\n",
    "            labels_list.append(label_ids)\n",
    "            text_ids_mask_list.append(text_mask)\n",
    "            image_ids_mask_list.append(image_mask)\n",
    "            label_text_ids_mask_list.append(label_text_mask)\n",
    "            label_image_ids_mask_list.append(label_image_mask)\n",
    "\n",
    "        # Padding 处理\n",
    "        input_ids = pad_sequence(input_ids_list, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        labels = pad_sequence(labels_list, batch_first=True, padding_value=self.IGNORE_INDEX)\n",
    "        text_ids_mask = pad_sequence(text_ids_mask_list, batch_first=True, padding_value=False)\n",
    "        image_ids_mask = pad_sequence(image_ids_mask_list, batch_first=True, padding_value=False)\n",
    "        label_text_ids_mask = pad_sequence(label_text_ids_mask_list, batch_first=True, padding_value=False)\n",
    "        label_image_ids_mask = pad_sequence(label_image_ids_mask_list, batch_first=True, padding_value=False)\n",
    "\n",
    "        # 截断处理\n",
    "        if input_ids.size(1) > self.max_length:\n",
    "            input_ids = input_ids[:, :self.max_length]\n",
    "            labels = labels[:, :self.max_length]\n",
    "            text_ids_mask = text_ids_mask[:, :self.max_length]\n",
    "            image_ids_mask = image_ids_mask[:, :self.max_length]\n",
    "            label_text_ids_mask = label_text_ids_mask[:, :self.max_length]\n",
    "            label_image_ids_mask = label_image_ids_mask[:, :self.max_length]\n",
    "\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=(input_ids != self.tokenizer.pad_token_id),\n",
    "            text_ids_mask=text_ids_mask,\n",
    "            image_ids_mask=image_ids_mask,\n",
    "            label_text_ids_mask=label_text_ids_mask,\n",
    "            label_image_ids_mask=label_image_ids_mask,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[100000,   2054,    418,  ..., 100015, 100015, 100015],\n",
       "         [100000,   2054,    418,  ..., 100015, 100015, 100015],\n",
       "         [100000,   2054,    418,  ...,   -100,   -100, 100593]]),\n",
       " 'labels': tensor([[  2054,    418,    274,  ...,   -100,   -100,   -100],\n",
       "         [  2054,    418,    274,  ...,   -100,   -100,   -100],\n",
       "         [  2054,    418,    274,  ...,   -100, 100593, 100001]]),\n",
       " 'attention_mask': tensor([[ True,  True,  True,  ..., False, False, False],\n",
       "         [ True,  True,  True,  ..., False, False, False],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True]]),\n",
       " 'text_ids_mask': tensor([[ True,  True,  True,  ..., False, False, False],\n",
       "         [ True,  True,  True,  ..., False, False, False],\n",
       "         [ True,  True,  True,  ..., False, False,  True]]),\n",
       " 'image_ids_mask': tensor([[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ...,  True,  True, False]]),\n",
       " 'label_text_ids_mask': tensor([[ True,  True,  True,  ..., False, False, False],\n",
       "         [ True,  True,  True,  ..., False, False, False],\n",
       "         [ True,  True,  True,  ..., False,  True,  True]]),\n",
       " 'label_image_ids_mask': tensor([[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ...,  True, False, False]])}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 假设你已经有一个 processor 实例\n",
    "collator = DataCollatorForSupervisedDataset(\n",
    "    tokenizer=processor.tokenizer,\n",
    "    processor=processor,\n",
    "    max_length=2048\n",
    ")\n",
    "\n",
    "# 测试一下\n",
    "batch = collator([train_dataset[1], train_dataset[2], train_dataset[3]])\n",
    "batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "352398"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches_per_epoch = len(train_dataset) // 8\n",
    "batches_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'get_tar_info'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tar_file, file_index \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tar_info\u001b[49m(\u001b[38;5;241m2132052\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m索引 2132052 的数据位于 tar 文件: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtar_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m，文件内第 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 个样本\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'get_tar_info'"
     ]
    }
   ],
   "source": [
    "tar_file, file_index = train_dataset.get_tar_info(2132052)\n",
    "print(f\"索引 2132052 的数据位于 tar 文件: {tar_file}，文件内第 {file_index} 个样本\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webdataset as wds\n",
    "\n",
    "shard_path = '/mnt/v-haodongli/cot_output_test_train/02166.tar'\n",
    "target_key = \"02166_00028\"  # 想看的 key\n",
    "\n",
    "dataset = wds.WebDataset(shard_path).decode().to_tuple(\"__key__\", \"json\")\n",
    "\n",
    "for key, label in dataset:\n",
    "    if key == target_key:\n",
    "        print(\"Key:\", key)\n",
    "        print(\"Label:\", label)\n",
    "        break  # 找到后退出循环   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor: VLChatProcessor = VLChatProcessor.from_pretrained(\"deepseek-ai/Janus-Pro-7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Sequence, Dict, Any\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset:\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "    tokenizer: PreTrainedTokenizer\n",
    "    max_length: int = 1024\n",
    "    IGNORE_INDEX: int = -100\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids_list = []\n",
    "        labels_list = []\n",
    "        text_ids_mask_list = []\n",
    "        image_ids_mask_list = []\n",
    "        label_text_ids_mask_list = []\n",
    "        label_image_ids_mask_list = []\n",
    "\n",
    "        for instance in instances:\n",
    "            # 提取 caption 和 img_index\n",
    "            try:\n",
    "                json_data = instance['json']\n",
    "                caption = json_data['caption']\n",
    "                img_index = json_data['img_index']  # list of int 或者 tensor\n",
    "            except KeyError as e:\n",
    "                raise ValueError(f\"Missing key in instance: {e}\")\n",
    "\n",
    "            # 构造 conversation\n",
    "            conversation = [\n",
    "                {\"role\": \"<|User|>\", \"content\": caption},\n",
    "                {\"role\": \"<|Assistant|>\", \"content\": f\"{caption}<begin_of_image><end_of_image>\"},\n",
    "            ]\n",
    "            system_prompt = \"You are an assistant that creates images from descriptions. First, describe the image in detail, then generate it.\"\n",
    "\n",
    "            # Tokenize prompt\n",
    "            prompt = self.tokenizer.apply_sft_template_for_multi_turn_prompts(\n",
    "                conversations=conversation,\n",
    "                sft_format=\"simple\",  # 根据你的 processor 修改\n",
    "                system_prompt=system_prompt,\n",
    "            )\n",
    "            text_ids = self.tokenizer.encode(prompt)\n",
    "\n",
    "            # 插入图像 token ID\n",
    "            all_ids = text_ids[:-2] + img_index + text_ids[-2:]\n",
    "            all_ids = torch.LongTensor(all_ids)\n",
    "\n",
    "            # 构建图像 token 的 mask\n",
    "            all_image_ids_mask = torch.zeros(len(all_ids), dtype=torch.bool)\n",
    "            all_image_ids_mask[-len(img_index)-2:-2] = True\n",
    "\n",
    "            # 找到 Assistant 回答开始的位置\n",
    "            try:\n",
    "                assistant_start_token_id = self.tokenizer.encode(\"<|Assistant|>\")[0]\n",
    "                assistant_start_index = (all_ids == assistant_start_token_id).nonzero(as_tuple=True)[0][0].item()\n",
    "            except Exception:\n",
    "                assistant_start_index = 0\n",
    "\n",
    "            # 构造各类 mask\n",
    "            assistant_mask = torch.zeros(len(all_ids), dtype=torch.bool)\n",
    "            assistant_mask[assistant_start_index:] = True\n",
    "\n",
    "            # 构造 input 和 label\n",
    "            input_ids = all_ids[:-1]\n",
    "            label_ids = all_ids[1:]\n",
    "\n",
    "            text_mask = (all_image_ids_mask[:-1] == False)\n",
    "            image_mask = all_image_ids_mask[:-1]\n",
    "\n",
    "            label_text_mask = assistant_mask[1:] & (all_image_ids_mask[1:] == False)\n",
    "            label_image_mask = assistant_mask[1:] & all_image_ids_mask[1:]\n",
    "\n",
    "            # 只保留 label 中需要的部分，其他设为 IGNORE_INDEX\n",
    "            label_ids[~label_text_mask] = self.IGNORE_INDEX\n",
    "\n",
    "            # 添加进列表\n",
    "            input_ids_list.append(input_ids)\n",
    "            labels_list.append(label_ids)\n",
    "            text_ids_mask_list.append(text_mask)\n",
    "            image_ids_mask_list.append(image_mask)\n",
    "            label_text_ids_mask_list.append(label_text_mask)\n",
    "            label_image_ids_mask_list.append(label_image_mask)\n",
    "\n",
    "        # Padding 处理\n",
    "        input_ids = pad_sequence(input_ids_list, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        labels = pad_sequence(labels_list, batch_first=True, padding_value=self.IGNORE_INDEX)\n",
    "        text_ids_mask = pad_sequence(text_ids_mask_list, batch_first=True, padding_value=False)\n",
    "        image_ids_mask = pad_sequence(image_ids_mask_list, batch_first=True, padding_value=False)\n",
    "        label_text_ids_mask = pad_sequence(label_text_ids_mask_list, batch_first=True, padding_value=False)\n",
    "        label_image_ids_mask = pad_sequence(label_image_ids_mask_list, batch_first=True, padding_value=False)\n",
    "\n",
    "        # 截断处理\n",
    "        if input_ids.size(1) > self.max_length:\n",
    "            input_ids = input_ids[:, :self.max_length]\n",
    "            labels = labels[:, :self.max_length]\n",
    "            text_ids_mask = text_ids_mask[:, :self.max_length]\n",
    "            image_ids_mask = image_ids_mask[:, :self.max_length]\n",
    "            label_text_ids_mask = label_text_ids_mask[:, :self.max_length]\n",
    "            label_image_ids_mask = label_image_ids_mask[:, :self.max_length]\n",
    "\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=(input_ids != self.tokenizer.pad_token_id),\n",
    "            text_ids_mask=text_ids_mask,\n",
    "            image_ids_mask=image_ids_mask,\n",
    "            label_text_ids_mask=label_text_ids_mask,\n",
    "            label_image_ids_mask=label_image_ids_mask,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/v-haodongli/miniconda3/envs/janus/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/v-haodongli/miniconda3/envs/janus/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n  File \"/tmp/ipykernel_9383/2233236194.py\", line 39, in __call__\n    prompt = self.tokenizer.apply_sft_template_for_multi_turn_prompts(\n  File \"/home/v-haodongli/miniconda3/envs/janus/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 1108, in __getattr__\n    raise AttributeError(f\"{self.__class__.__name__} has no attribute {key}\")\nAttributeError: LlamaTokenizerFast has no attribute apply_sft_template_for_multi_turn_prompts\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m collator \u001b[38;5;241m=\u001b[39m DataCollatorForSupervisedDataset(tokenizer\u001b[38;5;241m=\u001b[39mprocessor\u001b[38;5;241m.\u001b[39mtokenizer)\n\u001b[1;32m      3\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m      4\u001b[0m     train_dataset,\n\u001b[1;32m      5\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# 只打印第一个 batch\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/janus/lib/python3.10/site-packages/torch/utils/data/dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    739\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/janus/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1515\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1513\u001b[0m worker_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info\u001b[38;5;241m.\u001b[39mpop(idx)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworker_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/janus/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1550\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data, worker_idx)\u001b[0m\n\u001b[1;32m   1548\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1550\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/janus/lib/python3.10/site-packages/torch/_utils.py:750\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments or otherwise can't\u001b[39;00m\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;66;03m# be constructed, don't try to instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 750\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mAttributeError\u001b[0m: Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/v-haodongli/miniconda3/envs/janus/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/v-haodongli/miniconda3/envs/janus/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n  File \"/tmp/ipykernel_9383/2233236194.py\", line 39, in __call__\n    prompt = self.tokenizer.apply_sft_template_for_multi_turn_prompts(\n  File \"/home/v-haodongli/miniconda3/envs/janus/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 1108, in __getattr__\n    raise AttributeError(f\"{self.__class__.__name__} has no attribute {key}\")\nAttributeError: LlamaTokenizerFast has no attribute apply_sft_template_for_multi_turn_prompts\n"
     ]
    }
   ],
   "source": [
    "collator = DataCollatorForSupervisedDataset(tokenizer=processor.tokenizer)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    collate_fn=collator,\n",
    "    num_workers=8,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    break  # 只打印第一个 batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(instances, tokenizer, max_length=1024, IGNORE_INDEX=-100):\n",
    "    input_ids_list = []\n",
    "    labels_list = []\n",
    "    text_ids_mask_list = []\n",
    "    image_ids_mask_list = []\n",
    "    label_text_ids_mask_list = []\n",
    "    label_image_ids_mask_list = []\n",
    "\n",
    "    for instance in instances:\n",
    "        # 提取 caption 和 img_index\n",
    "        try:\n",
    "            json_data = instance['json']\n",
    "            caption = json_data['caption']\n",
    "            img_index = json_data['img_index']  # list of int 或者 tensor\n",
    "        except KeyError as e:\n",
    "            raise ValueError(f\"Missing key in instance: {e}\")\n",
    "\n",
    "        # 构造 conversation\n",
    "        conversation = [\n",
    "            {\"role\": \"<|User|>\", \"content\": caption},\n",
    "            {\"role\": \"<|Assistant|>\", \"content\": f\"{caption}<begin_of_image><end_of_image>\"},\n",
    "        ]\n",
    "        system_prompt = \"You are an assistant that creates images from descriptions. First, describe the image in detail, then generate it.\"\n",
    "\n",
    "        # Tokenize prompt\n",
    "        prompt = tokenizer.apply_sft_template_for_multi_turn_prompts(\n",
    "            conversations=conversation,\n",
    "            sft_format=\"simple\",  # 根据你的 processor 修改\n",
    "            system_prompt=system_prompt,\n",
    "        )\n",
    "        text_ids = tokenizer.encode(prompt)\n",
    "\n",
    "        # 插入图像 token ID\n",
    "        all_ids = text_ids[:-2] + img_index + text_ids[-2:]\n",
    "        all_ids = torch.LongTensor(all_ids)\n",
    "\n",
    "        # 构建图像 token 的 mask\n",
    "        all_image_ids_mask = torch.zeros(len(all_ids), dtype=torch.bool)\n",
    "        all_image_ids_mask[-len(img_index)-2:-2] = True\n",
    "\n",
    "        # 找到 Assistant 回答开始的位置\n",
    "        try:\n",
    "            assistant_start_token_id = tokenizer.encode(\"<|Assistant|>\")[0]\n",
    "            assistant_start_index = (all_ids == assistant_start_token_id).nonzero(as_tuple=True)[0][0].item()\n",
    "        except Exception:\n",
    "            assistant_start_index = 0\n",
    "\n",
    "        # 构造各类 mask\n",
    "        assistant_mask = torch.zeros(len(all_ids), dtype=torch.bool)\n",
    "        assistant_mask[assistant_start_index:] = True\n",
    "\n",
    "        # 构造 input 和 label\n",
    "        input_ids = all_ids[:-1]\n",
    "        label_ids = all_ids[1:]\n",
    "\n",
    "        text_mask = (all_image_ids_mask[:-1] == False)\n",
    "        image_mask = all_image_ids_mask[:-1]\n",
    "\n",
    "        label_text_mask = assistant_mask[1:] & (all_image_ids_mask[1:] == False)\n",
    "        label_image_mask = assistant_mask[1:] & all_image_ids_mask[1:]\n",
    "\n",
    "        # 只保留 label 中需要的部分，其他设为 IGNORE_INDEX\n",
    "        label_ids[~label_text_mask] = IGNORE_INDEX\n",
    "\n",
    "        # 添加进列表\n",
    "        input_ids_list.append(input_ids)\n",
    "        labels_list.append(label_ids)\n",
    "        text_ids_mask_list.append(text_mask)\n",
    "        image_ids_mask_list.append(image_mask)\n",
    "        label_text_ids_mask_list.append(label_text_mask)\n",
    "        label_image_ids_mask_list.append(label_image_mask)\n",
    "\n",
    "    # Padding 处理\n",
    "    input_ids = pad_sequence(input_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    labels = pad_sequence(labels_list, batch_first=True, padding_value=IGNORE_INDEX)\n",
    "    text_ids_mask = pad_sequence(text_ids_mask_list, batch_first=True, padding_value=False)\n",
    "    image_ids_mask = pad_sequence(image_ids_mask_list, batch_first=True, padding_value=False)\n",
    "    label_text_ids_mask = pad_sequence(label_text_ids_mask_list, batch_first=True, padding_value=False)\n",
    "    label_image_ids_mask = pad_sequence(label_image_ids_mask_list, batch_first=True, padding_value=False)\n",
    "\n",
    "    # 截断处理\n",
    "    if input_ids.size(1) > max_length:\n",
    "        input_ids = input_ids[:, :max_length]\n",
    "        labels = labels[:, :max_length]\n",
    "        text_ids_mask = text_ids_mask[:, :max_length]\n",
    "        image_ids_mask = image_ids_mask[:, :max_length]\n",
    "        label_text_ids_mask = label_text_ids_mask[:, :max_length]\n",
    "        label_image_ids_mask = label_image_ids_mask[:, :max_length]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels,\n",
    "        \"attention_mask\": (input_ids != tokenizer.pad_token_id),\n",
    "        \"text_ids_mask\": text_ids_mask,\n",
    "        \"image_ids_mask\": image_ids_mask,\n",
    "        \"label_text_ids_mask\": label_text_ids_mask,\n",
    "        \"label_image_ids_mask\": label_image_ids_mask,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DataCollatorForSupervisedDataset.__init__() missing 2 required positional arguments: 'tokenizer' and 'processor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m      2\u001b[0m     train_dataset,\n\u001b[1;32m      3\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m----> 4\u001b[0m     collate_fn\u001b[38;5;241m=\u001b[39m\u001b[43mDataCollatorForSupervisedDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m      5\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m      6\u001b[0m     pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: DataCollatorForSupervisedDataset.__init__() missing 2 required positional arguments: 'tokenizer' and 'processor'"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    collate_fn=DataCollatorForSupervisedDataset(),\n",
    "    num_workers=8,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_dataset)):\n",
    "    try:\n",
    "        sample = train_dataset[i]\n",
    "        if len(sample['input_ids'])>2000:\n",
    "            print(f\"Sample index {i}: input_ids length = {len(sample['input_ids'])}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error at index {i}: {e}\")\n",
    "        # 打印原始数据源信息\n",
    "        print(\"Raw data:\", train_dataset.data[i])\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = 2010347\n",
    "for i in range(start_index, len(train_dataset)):\n",
    "    try:\n",
    "        sample = train_dataset[i]\n",
    "        if len(sample['input_ids']) > 2000:  # 检查 input_ids 长度\n",
    "            print(f\"Sample index {i}: input_ids length = {len(sample['input_ids'])}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error at index {i}: {e}\")  # 打印错误信息\n",
    "        print(\"Raw data:\", train_dataset.data[i])  # 打印原始数据（需确保 data 属性存在）\n",
    "        continue  # 跳过当前错误样本，继续循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = 2132053\n",
    "for i in range(start_index, len(train_dataset)):\n",
    "    try:\n",
    "        sample = train_dataset[i]\n",
    "        if len(sample['input_ids']) > 2000:  # 检查 input_ids 长度\n",
    "            print(f\"Sample index {i}: input_ids length = {len(sample['input_ids'])}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error at index {i}: {e}\")  # 打印错误信息\n",
    "        print(\"Raw data:\", train_dataset.data[i])  # 打印原始数据（需确保 data 属性存在）\n",
    "        continue  # 跳过当前错误样本，继续循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webdataset as wds\n",
    "import os\n",
    "\n",
    "# 输入路径（同时也是目标输出路径）\n",
    "input_shard = '/mnt/v-haodongli/cot_output_test_train/02166.tar'\n",
    "\n",
    "# 临时中间文件路径\n",
    "temp_shard = input_shard + \".tmp\"\n",
    "\n",
    "# 想要删除的 key\n",
    "target_key_to_remove = \"02166_00027\"\n",
    "\n",
    "# 第一步：读取原始 tar，过滤后写入临时文件\n",
    "with wds.TarWriter(temp_shard) as sink:\n",
    "    with wds.WebDataset(input_shard) as dataset:\n",
    "        for sample in dataset:\n",
    "            if sample[\"__key__\"] == target_key_to_remove:\n",
    "                print(f\"Skipping key: {target_key_to_remove}\")\n",
    "                continue\n",
    "            sink.write(sample)\n",
    "\n",
    "# 第二步：将临时文件替换回原文件名（覆盖原文件）\n",
    "os.replace(temp_shard, input_shard)\n",
    "\n",
    "print(f\"Done. Removed key '{target_key_to_remove}' and overwritten the original file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in the saved file:\n",
      "dict_keys(['batch', 'global_step', 'input_ids', 'text_id_mask', 'image_id_mask', 'label_ids', 'label_text_id_mask', 'label_image_id_mask'])\n",
      "\n",
      "Data details:\n",
      "batch: <class 'dict'>\n",
      "  input_ids: torch.Size([4, 1176]) | dtype: torch.int64\n",
      "  label_ids: torch.Size([4, 1176]) | dtype: torch.int64\n",
      "  attention_mask: torch.Size([4, 1176]) | dtype: torch.bool\n",
      "  text_id_mask: torch.Size([4, 1176]) | dtype: torch.bool\n",
      "  image_id_mask: torch.Size([4, 1176]) | dtype: torch.bool\n",
      "  label_text_id_mask: torch.Size([4, 1176]) | dtype: torch.bool\n",
      "  label_image_id_mask: torch.Size([4, 1176]) | dtype: torch.bool\n",
      "global_step: <class 'int'>\n",
      "  Value: 326\n",
      "input_ids: torch.Size([4, 1176]) | dtype: torch.int64\n",
      "text_id_mask: torch.Size([4, 1176]) | dtype: torch.bool\n",
      "image_id_mask: torch.Size([4, 1176]) | dtype: torch.bool\n",
      "label_ids: torch.Size([4, 1176]) | dtype: torch.int64\n",
      "label_text_id_mask: torch.Size([4, 1176]) | dtype: torch.bool\n",
      "label_image_id_mask: torch.Size([4, 1176]) | dtype: torch.bool\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 文件路径\n",
    "file_path = \"/scratch/amlt_code/debug_batch_step_327_rank_3.pt\"\n",
    "\n",
    "# 加载文件\n",
    "data = torch.load(file_path, map_location='cpu')  # 建议先加载到 CPU 上\n",
    "\n",
    "# 打印所有 key\n",
    "print(\"Keys in the saved file:\")\n",
    "print(data.keys())\n",
    "\n",
    "# 打印每个 key 对应的数据形状或内容\n",
    "print(\"\\nData details:\")\n",
    "for key, value in data.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"{key}: {value.shape} | dtype: {value.dtype}\")\n",
    "    else:\n",
    "        print(f\"{key}: {type(value)}\")\n",
    "        if isinstance(value, dict):\n",
    "            for k, v in value.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    print(f\"  {k}: {v.shape} | dtype: {v.dtype}\")\n",
    "                else:\n",
    "                    print(f\"  {k}: {type(v)}\")\n",
    "        else:\n",
    "            print(f\"  Value: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ids = data[\"label_ids\"]\n",
    "label_text_ids_mask = data[\"label_text_id_mask\"]\n",
    "input_ids = data[\"input_ids\"]\n",
    "image_ids_mask = data[\"image_id_mask\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = processor.tokenizer.decode(label_ids[label_text_ids_mask], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an assistant that creates images from descriptions. First, describe the image in detail, then generate it.\\n\\n<|User|>: Valentines table runner\\n\\n<|Assistant|>: A Valentine\\'s table runner is displayed. It is rectangular in shape, with a red and white color scheme. The runner is composed of numerous small squares, each featuring a heart design. The hearts are red and white, with some having a three-dimensional texture. The squares are arranged in a grid pattern, creating a patchwork effect. The table runner is placed on a wooden surface, which is partially visible at the top and bottom edges of the image. The style of the image is a close-up photograph, focusing on the intricate details of the table runner.<begin_of_image><end_of_image><｜end▁of▁sentence｜>You are an assistant that creates images from descriptions. First, describe the image in detail, then generate it.\\n\\n<|User|>: The Coolest Cars In Racing Game History\\n\\n<|Assistant|>: The image depicts a racing game interface featuring a green and white sports car, the Viper Venom 1000. The car is shown in a dynamic pose, suggesting speed and motion. The background is a blurred cityscape, indicating the car is in motion. The interface includes various game statistics and information:\\n\\n- A blue bar at the top left corner, likely representing the player\\'s health or energy.\\n- A blue bar at the top right corner, possibly indicating the player\\'s speed or another game metric.\\n- A shield icon with the text \"Viper Venom 1000\" below it, indicating the car\\'s name.\\n- A list of attributes for the car: \"Мощность\" (Power), \"Скорость\" (Speed), and \"Управляемость\" (Handling).\\n- A blue bar at the bottom left corner, possibly representing the player\\'s current speed or another game metric.\\n- A blue bar at the bottom right corner, possibly indicating the player\\'s position in the race.\\n- A blue bar at the bottom center, possibly representing the player\\'s current speed or another game metric.\\n- A blue bar at the bottom center, possibly representing the player\\'s current speed or another game metric.\\n- A blue bar at the bottom center, possibly representing the player\\'s current speed or another game metric.\\n- A blue bar at the bottom center, possibly representing the player\\'s current speed or another game metric.\\n- A blue bar at the bottom center, possibly representing the player\\'s current speed or another game metric.\\n- A blue bar at the bottom center, possibly representing the player\\'s current speed or another game metric.\\n- A blue bar at the bottom center, possibly representing the player\\'s current speed or another game metric.\\n- A blue bar at the bottom center, possibly representing the player\\'s current speed or another game metric.\\n- A blue bar at the bottom center, possibly representing the player\\'s current speed or another game metric.\\n- A blue bar at the bottom center, possibly representing the player\\'s current speed or another game metric.\\n- A blue bar at the bottom center, possibly representing the player\\'s current speed or another game metric.\\n- A blue bar at the bottom center, possibly representing the player\\'s current speed or another game metric.\\n- A blue bar at the bottom center, possibly representing the player\\'s current speed or another game metric.\\n- A blue bar at the bottom center, possibly representing the player\\'s current speed or another game metric.\\n- A blue bar at the bottom center, possibly representing the<begin_of_image><end_of_image><｜end▁of▁sentence｜>You are an assistant that creates images from descriptions. First, describe the image in detail, then generate it.\\n\\n<|User|>: Check out the megayacht-filled Port de Monaco, where billionaires go to play.\\n\\n<|Assistant|>: The image depicts a picturesque harbor, Port de Monaco, filled with numerous megayachts. The yachts are docked in a bay surrounded by a rocky cliff. The harbor is nestled between the cliff and the sea, with a clear blue sky above. The yachts vary in size and color, with some appearing white and others in shades of blue and green. The cliff is covered with greenery, and there are buildings perched on the top of the cliff, overlooking the harbor. The sea is a vibrant blue, and the overall scene is bathed in sunlight, creating a serene and luxurious atmosphere. The style of the image is a high-resolution aerial photograph, capturing the beauty and opulence of the harbor.<begin_of_image><end_of_image><｜end▁of▁sentence｜>You are an assistant that creates images from descriptions. First, describe the image in detail, then generate it.\\n\\n<|User|>: My favorite fire pits {and why} via interior designer @fieldstonehill\\n\\n<|Assistant|>: The image shows a serene lakeside setting with a fire pit as the focal point. The fire pit is surrounded by a circular arrangement of red Adirondack chairs, each with a cushion. The chairs are positioned on a gravel base, which encircles the fire pit. The fire pit itself is made of stones and contains a small fire with smoke rising. A few items, including a cooler and a blanket, are placed around the fire pit. In the background, there is a grassy area leading to a dock where a small boat is moored. The water is calm, reflecting the sky, and the overall scene is bathed in the warm light of either sunrise or sunset. The style of the image is natural and inviting, emphasizing relaxation and outdoor enjoyment.<begin_of_image><end_of_image><｜end▁of▁sentence｜>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def decode_to_pil(vq_list, vl_gpt, shape=(1, 8, 24, 24)):\n",
    "    # 将列表转为张量并移动到GPU\n",
    "    vq_tensor = torch.tensor(vq_list, dtype=torch.int, device=\"cuda\")\n",
    "    print(vq_tensor.shape)\n",
    "    # 解码图像数据（假设vl_gpt已加载）\n",
    "    with torch.no_grad():\n",
    "        dec = vl_gpt.gen_vision_model.decode_code(vq_tensor, shape=shape)\n",
    "    \n",
    "    # 后处理：张量转图像\n",
    "    dec = dec.to(torch.float32).cpu().numpy().transpose(0, 2, 3, 1)\n",
    "    dec = np.clip((dec + 1) / 2 * 255, 0, 255).astype(np.uint8)\n",
    "    return Image.fromarray(dec[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2304])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 24, 24, 8]' is invalid for input of size 18432",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjanus\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_vlm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MultiModalityCausalLM\n\u001b[1;32m      2\u001b[0m model: MultiModalityCausalLM \u001b[38;5;241m=\u001b[39m MultiModalityCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeepseek-ai/Janus-Pro-7B\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m         trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mdecode_to_pil\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimage_ids_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m, in \u001b[0;36mdecode_to_pil\u001b[0;34m(vq_list, vl_gpt, shape)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 解码图像数据（假设vl_gpt已加载）\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 8\u001b[0m     dec \u001b[38;5;241m=\u001b[39m \u001b[43mvl_gpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen_vision_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvq_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 后处理：张量转图像\u001b[39;00m\n\u001b[1;32m     11\u001b[0m dec \u001b[38;5;241m=\u001b[39m dec\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/scratch/amlt_code/janus/models/vq_model.py:506\u001b[0m, in \u001b[0;36mVQModel.decode_code\u001b[0;34m(self, code_b, shape, channel_first)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode_code\u001b[39m(\u001b[38;5;28mself\u001b[39m, code_b, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, channel_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 506\u001b[0m     quant_b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_codebook_entry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannel_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m     dec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(quant_b)\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dec\n",
      "File \u001b[0;32m/scratch/amlt_code/janus/models/vq_model.py:294\u001b[0m, in \u001b[0;36mVectorQuantizer.get_codebook_entry\u001b[0;34m(self, indices, shape, channel_first)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m channel_first:\n\u001b[0;32m--> 294\u001b[0m         z_q \u001b[38;5;241m=\u001b[39m \u001b[43mz_q\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m         \u001b[38;5;66;03m# reshape back to match original input shape\u001b[39;00m\n\u001b[1;32m    296\u001b[0m         z_q \u001b[38;5;241m=\u001b[39m z_q\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 24, 24, 8]' is invalid for input of size 18432"
     ]
    }
   ],
   "source": [
    "from janus.models.modeling_vlm import MultiModalityCausalLM\n",
    "model: MultiModalityCausalLM = MultiModalityCausalLM.from_pretrained(\n",
    "        \"deepseek-ai/Janus-Pro-7B\",\n",
    "        trust_remote_code=True).to(\"cuda\")\n",
    "image = decode_to_pil(input_ids[image_ids_mask].tolist(), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in the saved file:\n",
      "dict_keys(['batch', 'global_step', 'input_ids', 'text_id_mask', 'image_id_mask', 'label_ids', 'label_text_id_mask', 'label_image_id_mask'])\n",
      "\n",
      "Data details:\n",
      "batch: <class 'dict'>\n",
      "  input_ids: torch.Size([4, 780]) | dtype: torch.int64\n",
      "  label_ids: torch.Size([4, 780]) | dtype: torch.int64\n",
      "  attention_mask: torch.Size([4, 780]) | dtype: torch.bool\n",
      "  text_id_mask: torch.Size([4, 780]) | dtype: torch.bool\n",
      "  image_id_mask: torch.Size([4, 780]) | dtype: torch.bool\n",
      "  label_text_id_mask: torch.Size([4, 780]) | dtype: torch.bool\n",
      "  label_image_id_mask: torch.Size([4, 780]) | dtype: torch.bool\n",
      "global_step: <class 'int'>\n",
      "  Value: 1\n",
      "input_ids: torch.Size([4, 780]) | dtype: torch.int64\n",
      "text_id_mask: torch.Size([4, 780]) | dtype: torch.bool\n",
      "image_id_mask: torch.Size([4, 780]) | dtype: torch.bool\n",
      "label_ids: torch.Size([4, 780]) | dtype: torch.int64\n",
      "label_text_id_mask: torch.Size([4, 780]) | dtype: torch.bool\n",
      "label_image_id_mask: torch.Size([4, 780]) | dtype: torch.bool\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 文件路径\n",
    "file_path = \"/scratch/amlt_code/debug_batch_step_2_rank_3.pt\"\n",
    "\n",
    "# 加载文件\n",
    "data = torch.load(file_path, map_location='cpu')  # 建议先加载到 CPU 上\n",
    "\n",
    "# 打印所有 key\n",
    "print(\"Keys in the saved file:\")\n",
    "print(data.keys())\n",
    "\n",
    "# 打印每个 key 对应的数据形状或内容\n",
    "print(\"\\nData details:\")\n",
    "for key, value in data.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"{key}: {value.shape} | dtype: {value.dtype}\")\n",
    "    else:\n",
    "        print(f\"{key}: {type(value)}\")\n",
    "        if isinstance(value, dict):\n",
    "            for k, v in value.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    print(f\"  {k}: {v.shape} | dtype: {v.dtype}\")\n",
    "                else:\n",
    "                    print(f\"  {k}: {type(v)}\")\n",
    "        else:\n",
    "            print(f\"  Value: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ids = data[\"label_ids\"]\n",
    "label_text_ids_mask = data[\"label_text_id_mask\"]\n",
    "input_ids = data[\"input_ids\"]\n",
    "image_ids_mask = data[\"image_id_mask\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2304])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 24, 24, 8]' is invalid for input of size 18432",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjanus\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_vlm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MultiModalityCausalLM\n\u001b[1;32m      2\u001b[0m model: MultiModalityCausalLM \u001b[38;5;241m=\u001b[39m MultiModalityCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeepseek-ai/Janus-Pro-7B\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m         trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mdecode_to_pil\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimage_ids_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m, in \u001b[0;36mdecode_to_pil\u001b[0;34m(vq_list, vl_gpt, shape)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 解码图像数据（假设vl_gpt已加载）\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 8\u001b[0m     dec \u001b[38;5;241m=\u001b[39m \u001b[43mvl_gpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen_vision_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvq_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 后处理：张量转图像\u001b[39;00m\n\u001b[1;32m     11\u001b[0m dec \u001b[38;5;241m=\u001b[39m dec\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/scratch/amlt_code/janus/models/vq_model.py:506\u001b[0m, in \u001b[0;36mVQModel.decode_code\u001b[0;34m(self, code_b, shape, channel_first)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode_code\u001b[39m(\u001b[38;5;28mself\u001b[39m, code_b, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, channel_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 506\u001b[0m     quant_b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_codebook_entry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannel_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m     dec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(quant_b)\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dec\n",
      "File \u001b[0;32m/scratch/amlt_code/janus/models/vq_model.py:294\u001b[0m, in \u001b[0;36mVectorQuantizer.get_codebook_entry\u001b[0;34m(self, indices, shape, channel_first)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m channel_first:\n\u001b[0;32m--> 294\u001b[0m         z_q \u001b[38;5;241m=\u001b[39m \u001b[43mz_q\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m         \u001b[38;5;66;03m# reshape back to match original input shape\u001b[39;00m\n\u001b[1;32m    296\u001b[0m         z_q \u001b[38;5;241m=\u001b[39m z_q\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 24, 24, 8]' is invalid for input of size 18432"
     ]
    }
   ],
   "source": [
    "from janus.models.modeling_vlm import MultiModalityCausalLM\n",
    "model: MultiModalityCausalLM = MultiModalityCausalLM.from_pretrained(\n",
    "        \"deepseek-ai/Janus-Pro-7B\",\n",
    "        trust_remote_code=True).to(\"cuda\")\n",
    "image = decode_to_pil(input_ids[image_ids_mask].tolist(), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"You are an assistant that creates images from descriptions. First, describe the image in detail, then generate it.\\n\\n<|User|>: 76 best cars coloring pages images on Pinterest Coloring sheets\\n\\n<|Assistant|>: The image is a black and white line drawing of a cartoon-style car. The car has a smiling face with eyes, a nose, and a mouth. It has a rounded body with a small, flat roof. The car is depicted with a single wheel on the front and a single wheel on the back, both of which are simple lines. The car is shown in a side profile, facing to the right. The car has a small, rectangular shape with a slightly curved front. The car is on a flat surface, and there are small, dashed lines indicating the ground beneath it. The style of the image is simple and cartoonish, with clean lines and no shading.<begin_of_image><end_of_image><｜end▁of▁sentence｜>You are an assistant that creates images from descriptions. First, describe the image in detail, then generate it.\\n\\n<|User|>: Healthier 2-Ingredient Peanut Butter Cups - You are going to love this easy peanut butter cup recipe! They're ready in less than 15 minutes (I know, how is this even possible?) and they taste great.\\n\\n<|Assistant|>: The image shows three chocolate-covered peanut butter cups stacked on top of each other. The cups are round with a smooth, glossy chocolate coating. The chocolate appears to be thick and slightly cracked in places, indicating a rich texture. The peanut butter inside is visible through the small opening at the top of each cup. The background is a plain, light gray, which contrasts with the dark chocolate and the creamy peanut butter, making the cups stand out. The style of the image is simple and clean, focusing on the product with a soft, diffused light that highlights the texture and color of the cups.<begin_of_image><end_of_image><｜end▁of▁sentence｜>You are an assistant that creates images from descriptions. First, describe the image in detail, then generate it.\\n\\n<|User|>: Watch the First Trailer for <i>Jobs</i>\\n\\n<|Assistant|>: The image shows a person with short, dark hair and a beard, wearing a light-colored, checkered button-up shirt. The background appears to be an indoor setting with a blurred, neutral-colored wall and a hint of a red object, possibly a candle or a piece of furniture, in the upper right corner. The style of the image is a close-up portrait with a shallow depth of field, focusing on the person's face and upper torso.<begin_of_image><end_of_image><｜end▁of▁sentence｜>You are an assistant that creates images from descriptions. First, describe the image in detail, then generate it.\\n\\n<|User|>: Take a dip in the Venetian Pool\\n\\n<|Assistant|>: The image depicts a Venetian Pool with clear turquoise water. Several people are swimming and lounging on the pool deck. The pool is surrounded by palm trees and a building with a red roof and white walls. The sky is blue with scattered white clouds. The pool deck is made of concrete and has a small island in the middle. The overall style is bright and sunny, suggesting a warm, tropical setting.<begin_of_image><end_of_image><｜end▁of▁sentence｜>\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from janus.models.processing_vlm import VLChatProcessor\n",
    "processor: VLChatProcessor = VLChatProcessor.from_pretrained(\"deepseek-ai/Janus-Pro-7B\")\n",
    "tokenizer = processor.tokenizer\n",
    "text = processor.tokenizer.decode(label_ids[label_text_ids_mask], skip_special_tokens=False)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "janus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
