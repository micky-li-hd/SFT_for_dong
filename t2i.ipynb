{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/v-haodongli/miniconda3/envs/janus/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Sequence\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import AutoProcessor\n",
    "import glob\n",
    "import transformers\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "data_files = glob.glob(os.path.join(\"/home/v-haodongli/mnt/v-haodongli-container/cot_output_test\", \"*.tar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Sequence\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import AutoProcessor\n",
    "import glob\n",
    "import transformers\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from dataclasses import dataclass\n",
    "from typing import Sequence, Dict, Any\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import PreTrainedTokenizer\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset:\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "    tokenizer: PreTrainedTokenizer\n",
    "    processor: Any  # 替换为你的具体 processor 类型（如 VLMProcessor）\n",
    "    max_length: int = 1024\n",
    "    IGNORE_INDEX: int = -100\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids_list = []\n",
    "        labels_list = []\n",
    "        text_ids_mask_list = []\n",
    "        image_ids_mask_list = []\n",
    "        label_text_ids_mask_list = []\n",
    "        label_image_ids_mask_list = []\n",
    "\n",
    "        for instance in instances:\n",
    "            # 提取 caption 和 img_index\n",
    "            try:\n",
    "                json_data = instance['json']\n",
    "                caption = json_data['caption']\n",
    "                cot = json_data['cot']  # 注意这里新增了 cot 字段\n",
    "                img_index = json_data['img_index']  # list of int 或者 tensor\n",
    "            except KeyError as e:\n",
    "                raise ValueError(f\"Missing key in instance: {e}\")\n",
    "\n",
    "            # 构造 conversation\n",
    "            conversation = [\n",
    "                {\"role\": \"<|User|>\", \"content\": caption},\n",
    "                {\"role\": \"<|Assistant|>\", \"content\": f\"{cot}<begin_of_image><end_of_image>\"},\n",
    "            ]\n",
    "            system_prompt = \"You are an assistant that creates images from descriptions. First, describe the image in detail, then generate it.\"\n",
    "\n",
    "            # 使用 self.processor 来生成 prompt\n",
    "            prompt = self.processor.apply_sft_template_for_multi_turn_prompts(\n",
    "                conversations=conversation,\n",
    "                sft_format=self.processor.sft_format,\n",
    "                system_prompt=system_prompt,\n",
    "            )\n",
    "\n",
    "            # Tokenize prompt\n",
    "            text_ids = self.tokenizer.encode(prompt)\n",
    "\n",
    "            # 插入图像 token ID\n",
    "            all_ids = text_ids[:-2] + img_index + text_ids[-2:]\n",
    "            all_ids = torch.LongTensor(all_ids)\n",
    "\n",
    "            # 构建图像 token 的 mask\n",
    "            all_image_ids_mask = torch.zeros(len(all_ids), dtype=torch.bool)\n",
    "            all_image_ids_mask[-len(img_index)-2:-2] = True\n",
    "\n",
    "            # 找到 Assistant 回答开始的位置\n",
    "            try:\n",
    "                assistant_start_token_id = self.tokenizer.encode(\"<|Assistant|>\")[0]\n",
    "                assistant_start_index = (all_ids == assistant_start_token_id).nonzero(as_tuple=True)[0][0].item()\n",
    "            except Exception:\n",
    "                assistant_start_index = 0\n",
    "\n",
    "            # 构造各类 mask\n",
    "            assistant_mask = torch.zeros(len(all_ids), dtype=torch.bool)\n",
    "            assistant_mask[assistant_start_index:] = True\n",
    "\n",
    "            # 构造 input 和 label\n",
    "            input_ids = all_ids[:-1]\n",
    "            label_ids = all_ids[1:]\n",
    "\n",
    "            text_mask = (all_image_ids_mask[:-1] == False)\n",
    "            image_mask = all_image_ids_mask[:-1]\n",
    "\n",
    "            label_text_mask = assistant_mask[1:] & (all_image_ids_mask[1:] == False)\n",
    "            label_image_mask = assistant_mask[1:] & all_image_ids_mask[1:]\n",
    "\n",
    "            # 只保留 label 中需要的部分，其他设为 IGNORE_INDEX\n",
    "            label_ids[~label_text_mask] = self.IGNORE_INDEX\n",
    "\n",
    "            # 添加进列表\n",
    "            input_ids_list.append(input_ids)\n",
    "            labels_list.append(label_ids)\n",
    "            text_ids_mask_list.append(text_mask)\n",
    "            image_ids_mask_list.append(image_mask)\n",
    "            label_text_ids_mask_list.append(label_text_mask)\n",
    "            label_image_ids_mask_list.append(label_image_mask)\n",
    "\n",
    "        # Padding 处理\n",
    "        input_ids = pad_sequence(input_ids_list, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        labels = pad_sequence(labels_list, batch_first=True, padding_value=self.IGNORE_INDEX)\n",
    "        text_ids_mask = pad_sequence(text_ids_mask_list, batch_first=True, padding_value=False)\n",
    "        image_ids_mask = pad_sequence(image_ids_mask_list, batch_first=True, padding_value=False)\n",
    "        label_text_ids_mask = pad_sequence(label_text_ids_mask_list, batch_first=True, padding_value=False)\n",
    "        label_image_ids_mask = pad_sequence(label_image_ids_mask_list, batch_first=True, padding_value=False)\n",
    "\n",
    "        # 截断处理\n",
    "        if input_ids.size(1) > self.max_length:\n",
    "            input_ids = input_ids[:, :self.max_length]\n",
    "            labels = labels[:, :self.max_length]\n",
    "            text_ids_mask = text_ids_mask[:, :self.max_length]\n",
    "            image_ids_mask = image_ids_mask[:, :self.max_length]\n",
    "            label_text_ids_mask = label_text_ids_mask[:, :self.max_length]\n",
    "            label_image_ids_mask = label_image_ids_mask[:, :self.max_length]\n",
    "\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            label_ids=labels,\n",
    "            attention_mask=(input_ids != self.tokenizer.pad_token_id),\n",
    "            text_id_mask=text_ids_mask,\n",
    "            image_id_mask=image_ids_mask,\n",
    "            label_text_id_mask=label_text_ids_mask,\n",
    "            label_image_id_mask=label_image_ids_mask,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\"webdataset\", data_files=data_files, split=\"train\",  streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IterableDataset({\n",
       "    features: ['json', '__key__', '__url__'],\n",
       "    num_shards: 3881\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from janus.models.processing_vlm import VLChatProcessor\n",
    "processor: VLChatProcessor = VLChatProcessor.from_pretrained(\"deepseek-ai/Janus-Pro-7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn=DataCollatorForSupervisedDataset(tokenizer=processor.tokenizer, processor=processor, max_length=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(train_dataset)  # 创建一次迭代器\n",
    "sample1 = next(train_iter)        # 第一个样本\n",
    "sample2 = next(train_iter)        # 第二个样本\n",
    "sample3 = next(train_iter)        # 第三个样本\n",
    "batch = [sample1, sample2, sample3]  # 包含不同样本的批次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(train_dataset)  # 创建一次迭代器\n",
    "sample4 = next(train_iter)        # 第一个样本\n",
    "sample5 = next(train_iter)        # 第二个样本\n",
    "sample6 = next(train_iter)        # 第三个样本\n",
    "batch2 = [sample4, sample5]  # 包含不同样本的批次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(train_dataset)  # 创建一次迭代器\n",
    "sample7 = next(train_iter)        # 第一个样本\n",
    "sample8 = next(train_iter)  \n",
    "sample9 = next(train_iter)  \n",
    "batch3 = [sample7]\n",
    "batch4 = [sample8]\n",
    "batch5 = [sample9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 775])\n"
     ]
    }
   ],
   "source": [
    "result = collate_fn(batch)\n",
    "print(result['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 775])\n"
     ]
    }
   ],
   "source": [
    "result = collate_fn(batch2)\n",
    "print(result['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = collate_fn(batch3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 775])\n"
     ]
    }
   ],
   "source": [
    "result = collate_fn(batch4)\n",
    "print(result['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 759])\n"
     ]
    }
   ],
   "source": [
    "result = collate_fn(batch5)\n",
    "print(result['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "janus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
