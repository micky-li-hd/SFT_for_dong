{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.01s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "import wandb\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True\n",
    ").to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "vl_gpt_tune: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/home/v-haodongli/mnt/v-haodongli-container_doch/haodongli/janus-SFT-cosine/checkpoint-220000/unwrapped_model\",\n",
    "    trust_remote_code=True\n",
    ").to(torch.bfloat16).cuda().eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def generate_text_then_image_with_cfg(\n",
    "    mmgpt: MultiModalityCausalLM,\n",
    "    vl_chat_processor: VLChatProcessor,\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 200,\n",
    "    image_token_num_per_image: int = 576,\n",
    "    temperature: float = 0.5,\n",
    "    cfg_weight: float = 5.0,\n",
    "    img_size: int = 384,\n",
    "    patch_size: int = 16,\n",
    "    parallel_size: int = 16,\n",
    "):\n",
    "    tokenizer = vl_chat_processor.tokenizer\n",
    "    device = mmgpt.device\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    begin_of_image_id = tokenizer.convert_tokens_to_ids(\"<begin_of_image>\")\n",
    "\n",
    "    # Step 1: ÊñáÊú¨ÁîüÊàêÈò∂ÊÆµÔºà‰∏ç‰ΩøÁî® CFGÔºâ\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    inputs_embeds = mmgpt.language_model.get_input_embeddings()(input_ids)\n",
    "\n",
    "    generated_text_tokens = []\n",
    "    past_key_values = None\n",
    "    is_generating_image = False\n",
    "    image_token_count = 0\n",
    "\n",
    "    print(\"üîç Starting text generation...\")\n",
    "    for step in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            outputs = mmgpt.language_model.model(\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                use_cache=True,\n",
    "                past_key_values=past_key_values\n",
    "            )\n",
    "            hidden_states = outputs.last_hidden_state\n",
    "            past_key_values = outputs.past_key_values\n",
    "\n",
    "        logits = mmgpt.language_model.lm_head(hidden_states[:, -1, :])\n",
    "        probs = torch.softmax(logits / temperature, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1).squeeze(0)\n",
    "\n",
    "        next_token_id = next_token.item()\n",
    "        generated_text_tokens.append(next_token_id)\n",
    "\n",
    "        if next_token_id == begin_of_image_id:\n",
    "            print(f\"üñºÔ∏è Detected <begin_of_image>, switching to image generation.\")\n",
    "            is_generating_image = True\n",
    "            break\n",
    "\n",
    "        inputs_embeds = mmgpt.language_model.get_input_embeddings()(next_token.unsqueeze(0))\n",
    "\n",
    "    assert is_generating_image, \"Model did not generate <begin_of_image>.\"\n",
    "    generated_text = tokenizer.decode(generated_text_tokens, skip_special_tokens=True)\n",
    "    print(f\"üìù Generated text: {generated_text}\")\n",
    "\n",
    "    # Step 2: ÊûÑÈÄ† condition/uncondition ËæìÂÖ•Áî®‰∫éÂõæÂÉèÁîüÊàê\n",
    "    cond_tokens = torch.cat([\n",
    "        input_ids[0],\n",
    "        torch.tensor(generated_text_tokens, dtype=torch.long, device=device),\n",
    "        torch.tensor([begin_of_image_id], dtype=torch.long, device=device)\n",
    "    ])\n",
    "\n",
    "    uncond_tokens = torch.cat([\n",
    "        input_ids[0][:1],\n",
    "        torch.tensor([vl_chat_processor.pad_id] * (len(cond_tokens) - 2), dtype=torch.long, device=device),\n",
    "        torch.tensor([begin_of_image_id], dtype=torch.long, device=device)\n",
    "    ])\n",
    "\n",
    "    cond_tokens = cond_tokens.unsqueeze(0).repeat(parallel_size, 1)\n",
    "    uncond_tokens = uncond_tokens.unsqueeze(0).repeat(parallel_size, 1)\n",
    "    tokens = torch.cat([cond_tokens, uncond_tokens], dim=0)\n",
    "    inputs_embeds = mmgpt.language_model.get_input_embeddings()(tokens)\n",
    "    past_key_values = None\n",
    "\n",
    "    generated_image_tokens = torch.zeros((parallel_size, image_token_num_per_image), dtype=torch.int, device=device)\n",
    "\n",
    "    print(\"üñºÔ∏è Starting image token generation with CFG...\")\n",
    "\n",
    "    for i in range(image_token_num_per_image):\n",
    "        with torch.no_grad():\n",
    "            outputs = mmgpt.language_model.model(\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                use_cache=True,\n",
    "                past_key_values=past_key_values\n",
    "            )\n",
    "            hidden_states = outputs.last_hidden_state\n",
    "            past_key_values = outputs.past_key_values\n",
    "\n",
    "        logits = mmgpt.gen_head(hidden_states[:, -1, :])\n",
    "        logit_cond = logits[0::2, :]\n",
    "        logit_uncond = logits[1::2, :]\n",
    "\n",
    "        logits = logit_uncond + cfg_weight * (logit_cond - logit_uncond)\n",
    "        probs = torch.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        generated_image_tokens[:, i] = next_token.squeeze(dim=-1)\n",
    "\n",
    "        next_token = torch.cat([next_token.unsqueeze(dim=1), next_token.unsqueeze(dim=1)], dim=1).view(-1)\n",
    "        img_embeds = mmgpt.prepare_gen_img_embeds(next_token)\n",
    "        inputs_embeds = img_embeds.unsqueeze(dim=1)\n",
    "\n",
    "    # Step 3: Ëß£Á†ÅÂõæÂÉè token ÊàêÂõæÂÉè\n",
    "    dec = mmgpt.gen_vision_model.decode_code(\n",
    "        generated_image_tokens.to(dtype=torch.int),\n",
    "        shape=[parallel_size, 8, img_size // patch_size, img_size // patch_size]\n",
    "    )\n",
    "\n",
    "    dec = dec.to(torch.float32).cpu().numpy().transpose(0, 2, 3, 1)\n",
    "    dec = np.clip((dec + 1) / 2 * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "    visual_img = np.zeros((parallel_size, img_size, img_size, 3), dtype=np.uint8)\n",
    "    visual_img[:, :, :] = dec\n",
    "\n",
    "    os.makedirs(\"step-220000\", exist_ok=True)\n",
    "    for i in range(parallel_size):\n",
    "        save_path = os.path.join(\"step-220000\", \"img_{}.jpg\".format(i))\n",
    "        PIL.Image.fromarray(visual_img[i]).save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Starting text generation...\n",
      "üñºÔ∏è Detected <begin_of_image>, switching to image generation.\n",
      "üìù Generated text:  A photo of a wooden bench with a dark brown, textured surface. The bench is positioned on a dark, paved surface. Behind the bench, there is a lush green bush and a flowering plant with white blossoms. The bench is illuminated by sunlight, creating a bright, warm glow. The overall style of the image is natural and serene, capturing a peaceful outdoor scene.\n",
      "üñºÔ∏è Starting image token generation with CFG...\n"
     ]
    }
   ],
   "source": [
    "# ÊûÑÂª∫ÂØπËØùÊ†ºÂºè\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"<|User|>\",\n",
    "        \"content\": \"The photo of a bench\",\n",
    "    },\n",
    "    {\"role\": \"<|Assistant|>\", \"content\": \"\"},\n",
    "]\n",
    "\n",
    "# Â∫îÁî® SFT Ê®°ÊùøÊ†ºÂºè\n",
    "sft_format = vl_chat_processor.apply_sft_template_for_multi_turn_prompts(\n",
    "    conversations=conversation,\n",
    "    sft_format=vl_chat_processor.sft_format,\n",
    "    system_prompt=\"You are a helpful assistant that can generate images based on text descriptions.\",\n",
    ")\n",
    "prompt = sft_format  # ‰∏çÂä† <begin_of_image>\n",
    "\n",
    "# ÁîüÊàêÂõæÂÉè\n",
    "generate_text_then_image_with_cfg(\n",
    "    mmgpt=vl_gpt_tune,\n",
    "    vl_chat_processor=vl_chat_processor,\n",
    "    prompt=prompt,\n",
    "    max_new_tokens=200,\n",
    "    image_token_num_per_image=576,\n",
    "    temperature=1,\n",
    "    cfg_weight=5.0,\n",
    "    parallel_size=1,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a helpful assistant that can generate images based on text descriptions.\\n\\n<|User|>: The photo of a bench\\n\\n<|Assistant|>:'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ÊûÑÂª∫ÂØπËØùÊ†ºÂºè\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"<|User|>\",\n",
    "        \"content\": \"The photo of a bench\",\n",
    "    },\n",
    "    {\"role\": \"<|Assistant|>\", \"content\": \"\"},\n",
    "]\n",
    "\n",
    "# Â∫îÁî® SFT Ê®°ÊùøÊ†ºÂºè\n",
    "sft_format = vl_chat_processor.apply_sft_template_for_multi_turn_prompts(\n",
    "    conversations=conversation,\n",
    "    sft_format=vl_chat_processor.sft_format,\n",
    "    system_prompt=\"You are a helpful assistant that can generate images based on text descriptions.\",\n",
    ")\n",
    "prompt = sft_format  # ‰∏çÂä† <begin_of_image>\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def generate(\n",
    "    mmgpt: MultiModalityCausalLM,\n",
    "    vl_chat_processor: VLChatProcessor,\n",
    "    prompt: str,\n",
    "    temperature: float = 1,\n",
    "    parallel_size: int = 16,\n",
    "    cfg_weight: float = 5,\n",
    "    image_token_num_per_image: int = 576,\n",
    "    img_size: int = 384,\n",
    "    patch_size: int = 16,\n",
    "):\n",
    "    input_ids = vl_chat_processor.tokenizer.encode(prompt)\n",
    "    input_ids = torch.LongTensor(input_ids)\n",
    "\n",
    "    tokens = torch.zeros((parallel_size*2, len(input_ids)), dtype=torch.int).cuda()\n",
    "    for i in range(parallel_size*2):\n",
    "        tokens[i, :] = input_ids\n",
    "        if i % 2 != 0:\n",
    "            tokens[i, 1:-1] = vl_chat_processor.pad_id\n",
    "\n",
    "    inputs_embeds = mmgpt.language_model.get_input_embeddings()(tokens)\n",
    "\n",
    "    generated_tokens = torch.zeros((parallel_size, image_token_num_per_image), dtype=torch.int).cuda()\n",
    "\n",
    "    for i in range(image_token_num_per_image):\n",
    "        outputs = mmgpt.language_model.model(inputs_embeds=inputs_embeds, use_cache=True, past_key_values=outputs.past_key_values if i != 0 else None)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        \n",
    "        logits = mmgpt.gen_head(hidden_states[:, -1, :])\n",
    "        logit_cond = logits[0::2, :]\n",
    "        logit_uncond = logits[1::2, :]\n",
    "        \n",
    "        logits = logit_uncond + cfg_weight * (logit_cond-logit_uncond)\n",
    "        probs = torch.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        generated_tokens[:, i] = next_token.squeeze(dim=-1)\n",
    "\n",
    "        next_token = torch.cat([next_token.unsqueeze(dim=1), next_token.unsqueeze(dim=1)], dim=1).view(-1)\n",
    "        img_embeds = mmgpt.prepare_gen_img_embeds(next_token)\n",
    "        inputs_embeds = img_embeds.unsqueeze(dim=1)\n",
    "\n",
    "\n",
    "    dec = mmgpt.gen_vision_model.decode_code(generated_tokens.to(dtype=torch.int), shape=[parallel_size, 8, img_size//patch_size, img_size//patch_size])\n",
    "    dec = dec.to(torch.float32).cpu().numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "    dec = np.clip((dec + 1) / 2 * 255, 0, 255)\n",
    "\n",
    "    visual_img = np.zeros((parallel_size, img_size, img_size, 3), dtype=np.uint8)\n",
    "    visual_img[:, :, :] = dec\n",
    "    \n",
    "    os.makedirs(prompt, exist_ok=True)\n",
    "    for i in range(parallel_size):\n",
    "        save_path = os.path.join(prompt, \"img_{}.jpg\".format(i))\n",
    "        PIL.Image.fromarray(visual_img[i]).save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(\n",
    "    mmgpt=vl_gpt,\n",
    "    vl_chat_processor=vl_chat_processor,\n",
    "    prompt=prompt + vl_chat_processor.image_start_tag,\n",
    "    image_token_num_per_image=576,\n",
    "    temperature=1,\n",
    "    cfg_weight=5.0,\n",
    "    parallel_size=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "janus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
