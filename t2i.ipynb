{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "import wandb\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True\n",
    ").to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "vl_gpt_tune: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/home/v-haodongli/mnt/v-haodongli-container_doch/haodongli/janus-SFT-cosine/checkpoint-220000/unwrapped_model\",\n",
    "    trust_remote_code=True\n",
    ").to(torch.bfloat16).cuda().eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def generate_text_then_image_with_cfg(\n",
    "    mmgpt: MultiModalityCausalLM,\n",
    "    vl_chat_processor: VLChatProcessor,\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 200,\n",
    "    image_token_num_per_image: int = 576,\n",
    "    temperature: float = 0.5,\n",
    "    cfg_weight: float = 5.0,\n",
    "    img_size: int = 384,\n",
    "    patch_size: int = 16,\n",
    "    parallel_size: int = 16,\n",
    "):\n",
    "    tokenizer = vl_chat_processor.tokenizer\n",
    "    device = mmgpt.device\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    begin_of_image_id = tokenizer.convert_tokens_to_ids(\"<begin_of_image>\")\n",
    "\n",
    "    # Step 1: 文本生成阶段（不使用 CFG）\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    inputs_embeds = mmgpt.language_model.get_input_embeddings()(input_ids)\n",
    "\n",
    "    generated_text_tokens = []\n",
    "    past_key_values = None\n",
    "    is_generating_image = False\n",
    "    image_token_count = 0\n",
    "\n",
    "    print(\"🔍 Starting text generation...\")\n",
    "    for step in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            outputs = mmgpt.language_model.model(\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                use_cache=True,\n",
    "                past_key_values=past_key_values\n",
    "            )\n",
    "            hidden_states = outputs.last_hidden_state\n",
    "            past_key_values = outputs.past_key_values\n",
    "\n",
    "        logits = mmgpt.language_model.lm_head(hidden_states[:, -1, :])\n",
    "        probs = torch.softmax(logits / temperature, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1).squeeze(0)\n",
    "\n",
    "        next_token_id = next_token.item()\n",
    "        generated_text_tokens.append(next_token_id)\n",
    "\n",
    "        if next_token_id == begin_of_image_id:\n",
    "            print(f\"🖼️ Detected <begin_of_image>, switching to image generation.\")\n",
    "            is_generating_image = True\n",
    "            break\n",
    "\n",
    "        inputs_embeds = mmgpt.language_model.get_input_embeddings()(next_token.unsqueeze(0))\n",
    "\n",
    "    assert is_generating_image, \"Model did not generate <begin_of_image>.\"\n",
    "    generated_text = tokenizer.decode(generated_text_tokens, skip_special_tokens=True)\n",
    "    print(f\"📝 Generated text: {generated_text}\")\n",
    "\n",
    "    # Step 2: 构造 condition/uncondition 输入用于图像生成\n",
    "    cond_tokens = torch.cat([\n",
    "        input_ids[0],\n",
    "        torch.tensor(generated_text_tokens, dtype=torch.long, device=device),\n",
    "        torch.tensor([begin_of_image_id], dtype=torch.long, device=device)\n",
    "    ])\n",
    "\n",
    "    uncond_tokens = torch.cat([\n",
    "        input_ids[0][:1],\n",
    "        torch.tensor([vl_chat_processor.pad_id] * (len(cond_tokens) - 2), dtype=torch.long, device=device),\n",
    "        torch.tensor([begin_of_image_id], dtype=torch.long, device=device)\n",
    "    ])\n",
    "\n",
    "    cond_tokens = cond_tokens.unsqueeze(0).repeat(parallel_size, 1)\n",
    "    uncond_tokens = uncond_tokens.unsqueeze(0).repeat(parallel_size, 1)\n",
    "    tokens = torch.cat([cond_tokens, uncond_tokens], dim=0)\n",
    "    inputs_embeds = mmgpt.language_model.get_input_embeddings()(tokens)\n",
    "    past_key_values = None\n",
    "\n",
    "    generated_image_tokens = torch.zeros((parallel_size, image_token_num_per_image), dtype=torch.int, device=device)\n",
    "\n",
    "    print(\"🖼️ Starting image token generation with CFG...\")\n",
    "\n",
    "    for i in range(image_token_num_per_image):\n",
    "        with torch.no_grad():\n",
    "            outputs = mmgpt.language_model.model(\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                use_cache=True,\n",
    "                past_key_values=past_key_values\n",
    "            )\n",
    "            hidden_states = outputs.last_hidden_state\n",
    "            past_key_values = outputs.past_key_values\n",
    "\n",
    "        logits = mmgpt.gen_head(hidden_states[:, -1, :])\n",
    "        logit_cond = logits[0::2, :]\n",
    "        logit_uncond = logits[1::2, :]\n",
    "\n",
    "        logits = logit_uncond + cfg_weight * (logit_cond - logit_uncond)\n",
    "        probs = torch.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        generated_image_tokens[:, i] = next_token.squeeze(dim=-1)\n",
    "\n",
    "        next_token = torch.cat([next_token.unsqueeze(dim=1), next_token.unsqueeze(dim=1)], dim=1).view(-1)\n",
    "        img_embeds = mmgpt.prepare_gen_img_embeds(next_token)\n",
    "        inputs_embeds = img_embeds.unsqueeze(dim=1)\n",
    "\n",
    "    # Step 3: 解码图像 token 成图像\n",
    "    dec = mmgpt.gen_vision_model.decode_code(\n",
    "        generated_image_tokens.to(dtype=torch.int),\n",
    "        shape=[parallel_size, 8, img_size // patch_size, img_size // patch_size]\n",
    "    )\n",
    "\n",
    "    dec = dec.to(torch.float32).cpu().numpy().transpose(0, 2, 3, 1)\n",
    "    dec = np.clip((dec + 1) / 2 * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "    visual_img = np.zeros((parallel_size, img_size, img_size, 3), dtype=np.uint8)\n",
    "    visual_img[:, :, :] = dec\n",
    "\n",
    "    os.makedirs(\"step-220000\", exist_ok=True)\n",
    "    for i in range(parallel_size):\n",
    "        save_path = os.path.join(\"step-220000\", \"img_{}.jpg\".format(i))\n",
    "        PIL.Image.fromarray(visual_img[i]).save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Starting text generation...\n",
      "🖼️ Detected <begin_of_image>, switching to image generation.\n",
      "📝 Generated text:  A photo of a wooden bench with a dark brown, textured surface. The bench is positioned on a dark, paved surface. Behind the bench, there is a lush green bush and a flowering plant with white blossoms. The bench is illuminated by sunlight, creating a bright, warm glow. The overall style of the image is natural and serene, capturing a peaceful outdoor scene.\n",
      "🖼️ Starting image token generation with CFG...\n"
     ]
    }
   ],
   "source": [
    "# 构建对话格式\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"<|User|>\",\n",
    "        \"content\": \"The photo of a bench\",\n",
    "    },\n",
    "    {\"role\": \"<|Assistant|>\", \"content\": \"\"},\n",
    "]\n",
    "\n",
    "# 应用 SFT 模板格式\n",
    "sft_format = vl_chat_processor.apply_sft_template_for_multi_turn_prompts(\n",
    "    conversations=conversation,\n",
    "    sft_format=vl_chat_processor.sft_format,\n",
    "    system_prompt=\"You are a helpful assistant that can generate images based on text descriptions.\",\n",
    ")\n",
    "prompt = sft_format  # 不加 <begin_of_image>\n",
    "\n",
    "# 生成图像\n",
    "generate_text_then_image_with_cfg(\n",
    "    mmgpt=vl_gpt_tune,\n",
    "    vl_chat_processor=vl_chat_processor,\n",
    "    prompt=prompt,\n",
    "    max_new_tokens=200,\n",
    "    image_token_num_per_image=576,\n",
    "    temperature=1,\n",
    "    cfg_weight=5.0,\n",
    "    parallel_size=1,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a helpful assistant that can generate images based on text descriptions.\\n\\n<|User|>: The photo of a bench\\n\\n<|Assistant|>:'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建对话格式\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"<|User|>\",\n",
    "        \"content\": \"The photo of a bench\",\n",
    "    },\n",
    "    {\"role\": \"<|Assistant|>\", \"content\": \"\"},\n",
    "]\n",
    "\n",
    "# 应用 SFT 模板格式\n",
    "sft_format = vl_chat_processor.apply_sft_template_for_multi_turn_prompts(\n",
    "    conversations=conversation,\n",
    "    sft_format=vl_chat_processor.sft_format,\n",
    "    system_prompt=\"You are a helpful assistant that can generate images based on text descriptions.\",\n",
    ")\n",
    "prompt = sft_format  # 不加 <begin_of_image>\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def generate(\n",
    "    mmgpt: MultiModalityCausalLM,\n",
    "    vl_chat_processor: VLChatProcessor,\n",
    "    prompt: str,\n",
    "    temperature: float = 1,\n",
    "    parallel_size: int = 16,\n",
    "    cfg_weight: float = 5,\n",
    "    image_token_num_per_image: int = 576,\n",
    "    img_size: int = 384,\n",
    "    patch_size: int = 16,\n",
    "):\n",
    "    input_ids = vl_chat_processor.tokenizer.encode(prompt)\n",
    "    input_ids = torch.LongTensor(input_ids)\n",
    "\n",
    "    tokens = torch.zeros((parallel_size*2, len(input_ids)), dtype=torch.int).cuda()\n",
    "    for i in range(parallel_size*2):\n",
    "        tokens[i, :] = input_ids\n",
    "        if i % 2 != 0:\n",
    "            tokens[i, 1:-1] = vl_chat_processor.pad_id\n",
    "\n",
    "    inputs_embeds = mmgpt.language_model.get_input_embeddings()(tokens)\n",
    "\n",
    "    generated_tokens = torch.zeros((parallel_size, image_token_num_per_image), dtype=torch.int).cuda()\n",
    "\n",
    "    for i in range(image_token_num_per_image):\n",
    "        outputs = mmgpt.language_model.model(inputs_embeds=inputs_embeds, use_cache=True, past_key_values=outputs.past_key_values if i != 0 else None)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        \n",
    "        logits = mmgpt.gen_head(hidden_states[:, -1, :])\n",
    "        logit_cond = logits[0::2, :]\n",
    "        logit_uncond = logits[1::2, :]\n",
    "        \n",
    "        logits = logit_uncond + cfg_weight * (logit_cond-logit_uncond)\n",
    "        probs = torch.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        generated_tokens[:, i] = next_token.squeeze(dim=-1)\n",
    "\n",
    "        next_token = torch.cat([next_token.unsqueeze(dim=1), next_token.unsqueeze(dim=1)], dim=1).view(-1)\n",
    "        img_embeds = mmgpt.prepare_gen_img_embeds(next_token)\n",
    "        inputs_embeds = img_embeds.unsqueeze(dim=1)\n",
    "\n",
    "\n",
    "    dec = mmgpt.gen_vision_model.decode_code(generated_tokens.to(dtype=torch.int), shape=[parallel_size, 8, img_size//patch_size, img_size//patch_size])\n",
    "    dec = dec.to(torch.float32).cpu().numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "    dec = np.clip((dec + 1) / 2 * 255, 0, 255)\n",
    "\n",
    "    visual_img = np.zeros((parallel_size, img_size, img_size, 3), dtype=np.uint8)\n",
    "    visual_img[:, :, :] = dec\n",
    "    \n",
    "    os.makedirs(prompt, exist_ok=True)\n",
    "    for i in range(parallel_size):\n",
    "        save_path = os.path.join(prompt, \"img_{}.jpg\".format(i))\n",
    "        PIL.Image.fromarray(visual_img[i]).save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(\n",
    "    mmgpt=vl_gpt,\n",
    "    vl_chat_processor=vl_chat_processor,\n",
    "    prompt=prompt + vl_chat_processor.image_start_tag,\n",
    "    image_token_num_per_image=576,\n",
    "    temperature=1,\n",
    "    cfg_weight=5.0,\n",
    "    parallel_size=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed log files:\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00151.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00152.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00153.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00164.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00168.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00169.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00170.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00172.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00177.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00183.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00188.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00190.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00191.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00192.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00193.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00194.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00195.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00196.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00197.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00199.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00375.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00376.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00388.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00397.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00489.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00567.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00571.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00572.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00585.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00587.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00588.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00597.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00669.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00671.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00676.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00680.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00684.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00691.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00697.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00698.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00699.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00747.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00748.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00751.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00755.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00765.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00769.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00781.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00782.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00783.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00784.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00785.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00786.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00787.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00788.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00789.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00790.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00791.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00792.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00793.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00794.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00795.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00796.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00797.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00798.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00799.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00855.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00859.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00863.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00868.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00872.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00880.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00881.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00882.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00883.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00884.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00885.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00886.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00887.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00888.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00889.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00890.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00891.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00892.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00893.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00894.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00895.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00896.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00897.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00898.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/00899.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/01190.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/01195.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/01196.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/01295.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/01297.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/01312.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/01318.tar\n",
      "[FAILED] /home/v-haodongli/mnt/v-haodongli-container/cot_output_test/01319.tar\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# 检查 log 文件是否包含 \"failed\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlog_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     22\u001b[0m         content \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m content\u001b[38;5;241m.\u001b[39mlower():\n",
      "File \u001b[0;32m~/miniconda3/envs/janus/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/janus/lib/python3.10/codecs.py:309\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.__init__\u001b[0;34m(self, errors)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBufferedIncrementalDecoder\u001b[39;00m(IncrementalDecoder):\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;124;03m    This subclass of IncrementalDecoder can be used as the baseclass for an\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;124;03m    incremental decoder if the decoder must be able to handle incomplete\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;124;03m    byte sequences.\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    310\u001b[0m         IncrementalDecoder\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, errors)\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;66;03m# undecoded input that is kept between calls to decode()\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 设置两个目录路径\n",
    "tar_dir = \"/home/v-haodongli/mnt/v-haodongli-container/cot_output_test\"\n",
    "log_dir = \"/home/v-haodongli/mnt/v-haodongli-container/status\"\n",
    "\n",
    "# 获取所有 .log 文件\n",
    "log_files = [f for f in os.listdir(log_dir) if f.endswith('.log')]\n",
    "\n",
    "print(\"Failed log files:\")\n",
    "for log_file in log_files:\n",
    "    base_name = os.path.splitext(log_file)[0]  # 去掉 .log，比如得到 \"00001\"\n",
    "    log_file_path = os.path.join(log_dir, log_file)\n",
    "\n",
    "    # 构造对应的 .tar 文件名\n",
    "    tar_file = f\"{base_name}.tar\"\n",
    "    tar_file_path = os.path.join(tar_dir, tar_file)\n",
    "\n",
    "    # 检查 log 文件是否包含 \"failed\"\n",
    "    try:\n",
    "        with open(log_file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            if \"failed\" in content.lower():\n",
    "                print(f\"[FAILED] {tar_file_path}\")  # 实时打印\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {log_file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "janus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
